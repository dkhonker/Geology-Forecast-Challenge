{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95697,"databundleVersionId":11372669,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom multiprocessing import Pool, cpu_count","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nimport glob\nimport random\nimport hashlib\nimport numpy as np\nimport pandas as pd\nimport time\nimport os\n\nNEGATIVE_PART = -299\nLARGEST_CHUNK = 600\nSMALLEST_CHUNK = 350\nTOTAL_REALIZATIONS = 10\n\ndef remove_chunk_opt(array, length):\n    if length > len(array):\n        raise ValueError(\"Length exceeds the size of the input array.\")\n    return array[:length], array[length:]\n\ndef add_chunk_to_rows(rows, input_array, chunk_length, file_name, initial_length):\n    position = initial_length - len(input_array)\n    chunk, shortened_array = remove_chunk_opt(input_array, chunk_length)\n    chunk = chunk - chunk[-(LARGEST_CHUNK + NEGATIVE_PART)]\n    padding_length = LARGEST_CHUNK - chunk_length\n    padded_chunk = np.concatenate((np.full(padding_length, np.nan), chunk))\n    full_id = f\"{file_name}_{position}\"\n    hash_hex_id = f\"g_{hashlib.md5(full_id.encode('utf-8')).hexdigest()[:10]}\"\n    row = [hash_hex_id] + padded_chunk.tolist()\n    rows.append(row)\n    return shortened_array\n\ndef process_folder_optimized(path_to_process, output_file_name=None, DO_PLOT=False, my_rnd=None, random_state=0):\n    if my_rnd is None:\n        my_rnd = random.Random()\n    csv_files = glob.glob(f\"{path_to_process}/*.csv\")\n    rows = []\n\n    for file_path in csv_files:\n        file_name = os.path.basename(file_path)\n        df = pd.read_csv(file_path)\n        new_vs_grid = np.arange(df['VS_APPROX_adjusted'].min(),\n                                df['VS_APPROX_adjusted'].max() + 1,\n                                1)\n        new_horizon_z = np.interp(new_vs_grid,\n                                  df['VS_APPROX_adjusted'],\n                                  df['HORIZON_Z_adjusted'])\n\n        remaining_array = new_horizon_z\n        initial_len = len(remaining_array)\n        total_large = len(remaining_array) // LARGEST_CHUNK // 2\n        for _ in range(total_large):\n            remaining_array = add_chunk_to_rows(rows, remaining_array, LARGEST_CHUNK, file_name, initial_len)\n\n        while len(remaining_array) >= LARGEST_CHUNK * 2.5:\n            chunk_len = my_rnd.randint(SMALLEST_CHUNK, LARGEST_CHUNK)\n            remaining_array = add_chunk_to_rows(rows, remaining_array, chunk_len, file_name, initial_len)\n\n        remaining_len = len(remaining_array) // 3\n        for _ in range(2):\n            remaining_array = add_chunk_to_rows(rows, remaining_array, remaining_len, file_name, initial_len)\n\n        remaining_array = add_chunk_to_rows(rows, remaining_array, len(remaining_array), file_name, initial_len)\n\n    columns = ['geology_id'] + [NEGATIVE_PART + i for i in range(LARGEST_CHUNK)]\n    total_df = pd.DataFrame(rows, columns=columns)\n    for k in range(1, TOTAL_REALIZATIONS):\n        for i in range(1, LARGEST_CHUNK + NEGATIVE_PART):\n            total_df[f\"r_{k}_pos_{i}\"] = total_df[i]\n\n    if output_file_name is not None:\n        total_df.to_csv(output_file_name, encoding='utf-8', index=False)\n    return total_df\n\n##The above is the code provided by the organizer\n\ndef run_until_no_new_rows(path_to_process, max_run_seconds=6*3600, output_file_name_prefix='train_optimized'):\n    my_rnd = random.Random(42)\n    merged_df = pd.DataFrame()\n    start_time = time.time()\n    iteration = 0\n    \n    # create 4 kernel pool\n    with Pool(processes=4) as pool:  # use pool\n        while True:\n            try:\n                iteration += 1\n                print(f\"Iteration {iteration} is running...\")\n                \n                # different seed\n                seeds = [my_rnd.randint(0, 2**32-1) for _ in range(4)]\n                \n                # Prepare multi-process parameters (do not save files, return DataFrame directly)\n                args_list = [(path_to_process, None, False, random.Random(seed)) for seed in seeds]\n                \n                # run multi process_folder_optimized\n                results = pool.starmap(process_folder_optimized, args_list)\n                # Merge all new rows in the result\n                combined_new = pd.DataFrame()\n                for new_df in results:\n                    if merged_df.empty:\n                        new_rows = new_df\n                    else:\n                        new_rows = new_df[~new_df['geology_id'].isin(merged_df['geology_id'])]\n                    combined_new = pd.concat([combined_new, new_rows], ignore_index=True)\n                \n                # Update merge results\n                if not combined_new.empty:\n                    merged_df = pd.concat([merged_df, combined_new], ignore_index=True)\n                \n                print(f\"Iteration {iteration}：Add unique rows = {len(combined_new)}，Total number of merged rows = {len(merged_df)}\")\n                \n                if combined_new.empty:\n                    print(\"success\")\n                    break\n                if (time.time() - start_time) > max_run_seconds:\n                    print(f\"Time is {max_run_seconds/3600:.2f} h. Stop.\")\n                    break\n            except Exception as e:\n                print(f\"Error: {e}. Skiped\")\n                continue\n\n    final_output = f\"{output_file_name_prefix}_final.csv\"\n    merged_df.to_csv(final_output, encoding='utf-8', index=False)\n    print(f\"Final result is in {final_output}\")\n    return merged_df\n\nmerged_result = run_until_no_new_rows(path_to_process='/kaggle/input/geology-forecast-challenge-open/data/train_raw',max_run_seconds=4*3600)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}