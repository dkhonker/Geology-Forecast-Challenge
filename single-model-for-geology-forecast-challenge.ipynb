{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":95697,"databundleVersionId":11372669,"sourceType":"competition"},{"sourceId":230001460,"sourceType":"kernelVersion"}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[Explanation of the Solution](https://www.kaggle.com/competitions/geology-forecast-challenge-open/discussion/582761)\n\n**Code:**\n\n[\"Generate dataset\"](https://www.kaggle.com/code/act18l/generate-dataset)\n\n[\"Single Model\"](https://www.kaggle.com/code/act18l/single-model-for-geology-forecast-challenge) \n\nThe dataset [\"geo_submission\"](https://www.kaggle.com/datasets/act18l/geo-submission/data) contains  the results from  versions of the above notebooks.\n\n\n\n[\"Emsemble\"](https://www.kaggle.com/code/act18l/ensemble-for-geology-forecast-challenge)\n\n---\n\nIn the competition, I chose to  **\"Single Model(Version 25)\"** and **\"Ensemble(Version 9)\"** as my submission.\n\nYou can use \"Compare Versions\" to check the code changes.\n\n---\n\n**Updated:** Cleaned up the code and added comments.","metadata":{}},{"cell_type":"markdown","source":"## Import Library","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom scipy.special import logsumexp\nimport os\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport warnings\n \nwarnings.filterwarnings(\"ignore\")\n# fix seed\n# seed = 42\n# os.environ['PYTHONHASHSEED'] = str(seed)  # fix python\n# random.seed(seed)                       # fix python\n# np.random.seed(seed)                    # fix numpy\n# tf.random.set_seed(seed)                # fix tensorflow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Read Data","metadata":{}},{"cell_type":"code","source":"%%time\nimport pandas as pd\nimport random\n\n#Randomly read 210,000 rows.\nfilename = \"/kaggle/input/notebook2241bf09e2/train_optimized_final.csv\"\nn_to_sample = 210000\n\nnum_lines = 314360\n\nif num_lines <= 0:\n    print(\"Empty.\")\nelse:\n    if n_to_sample > num_lines:\n        print(f\" ({n_to_sample}) > ({num_lines}).Will read all rows.\")\n        df = pd.read_csv(filename)\n    else:\n        all_indices = list(range(num_lines))\n        sample_indices = sorted(random.sample(all_indices, n_to_sample))\n        skip_rows = sorted(random.sample(range(1, num_lines + 1), num_lines - n_to_sample))\n        rows_to_keep_0_indexed = sorted(random.sample(range(num_lines), n_to_sample))\n        merge_df = pd.read_csv(filename,\n                         skiprows=lambda i: i > 0 and i -1 not in rows_to_keep_0_indexed).fillna(0)\n    print(f\"Read {len(merge_df)} rows.\")\n    print(merge_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T15:21:21.100314Z","iopub.execute_input":"2025-05-25T15:21:21.100877Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save space","metadata":{}},{"cell_type":"code","source":"%%time \nmerge_df['geology_id'] = 0 #we don't need geology_id.\nmerge_df['geology_id'] = merge_df['geology_id'].astype(\"category\")#To save space, we convert them to the category type.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Compress the integer and float numerical types.\nimport pandas as pd\ndef downcast_all(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    int_cols = df.select_dtypes(include=['int64', 'int32']).columns\n    df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast='integer')\n\n    float_cols = df.select_dtypes(include=['float64', 'float32']).columns\n    df[float_cols] = df[float_cols].apply(pd.to_numeric, downcast='float')\n\n    return df\n\nprint(\"Before: {:.2f} MB\".format(merge_df.memory_usage(deep=True).sum() / 1024**2))\nmerge_df = downcast_all(merge_df)\nprint(\"After : {:.2f} MB\".format(merge_df.memory_usage(deep=True).sum() / 1024**2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:51:45.847841Z","iopub.execute_input":"2025-04-24T04:51:45.848114Z","iopub.status.idle":"2025-04-24T04:52:04.176345Z","shell.execute_reply.started":"2025-04-24T04:51:45.848090Z","shell.execute_reply":"2025-04-24T04:52:04.175425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Read and merge","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/geology-forecast-challenge-open/data/train.csv\").fillna(0)\ntest=pd.read_csv(\"/kaggle/input/geology-forecast-challenge-open/data/test.csv\").fillna(0)\nsub=pd.read_csv('/kaggle/input/geology-forecast-challenge-open/data/sample_submission.csv')\ntrain.shape,test.shape,sub.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:52:04.177738Z","iopub.execute_input":"2025-04-24T04:52:04.178000Z","iopub.status.idle":"2025-04-24T04:52:06.410362Z","shell.execute_reply.started":"2025-04-24T04:52:04.177978Z","shell.execute_reply":"2025-04-24T04:52:06.409403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['geology_id'] = 0\ntest['geology_id'] = 0\ntrain['geology_id'] = train['geology_id'].astype(\"category\")\ntest['geology_id'] = test['geology_id'].astype(\"category\")\ntrain = downcast_all(train)\ntest = downcast_all(test)\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:52:06.411459Z","iopub.execute_input":"2025-04-24T04:52:06.411834Z","iopub.status.idle":"2025-04-24T04:52:07.900881Z","shell.execute_reply.started":"2025-04-24T04:52:06.411799Z","shell.execute_reply":"2025-04-24T04:52:07.900075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merge_df = pd.concat([train,merge_df],axis=0)\ntrain = merge_df\ntrain.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:52:07.901758Z","iopub.execute_input":"2025-04-24T04:52:07.901996Z","iopub.status.idle":"2025-04-24T04:52:08.546350Z","shell.execute_reply.started":"2025-04-24T04:52:07.901977Z","shell.execute_reply":"2025-04-24T04:52:08.545641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES=[c for c in test.columns if c!='geology_id']\nTARGETS=[c for c in sub.columns if c!='geology_id']\nsolution=train[['geology_id']+TARGETS].copy()\ntrain_sub=train[['geology_id']+TARGETS].copy()\ntrain_sub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:52:08.547210Z","iopub.execute_input":"2025-04-24T04:52:08.547428Z","iopub.status.idle":"2025-04-24T04:52:17.591535Z","shell.execute_reply.started":"2025-04-24T04:52:08.547410Z","shell.execute_reply":"2025-04-24T04:52:17.590761Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Word2Vec","metadata":{}},{"cell_type":"code","source":"VECTOR_SIZE = 0 #we don't use Word2Vec\nif VECTOR_SIZE==0:\n    train = train.drop('geology_id',axis=1)\n    test = test.drop('geology_id',axis=1)\n    train.head()\nelse:\n    from gensim.models import Word2Vec\n\n    sentences = [train['geology_id'].tolist(),test['geology_id'].tolist()]  \n\n    model = Word2Vec(sentences, vector_size=VECTOR_SIZE, window=10, min_count=1, sg=1)\n    \n    train = pd.concat([pd.DataFrame(train['geology_id'].apply(lambda x: model.wv[x]).tolist(), columns=[f'vec_{i}' for i in range(VECTOR_SIZE)]),train],axis=1).drop(\"geology_id\",axis=1)\n    test = pd.concat([pd.DataFrame(test['geology_id'].apply(lambda x: model.wv[x]).tolist(), columns=[f'vec_{i}' for i in range(VECTOR_SIZE)]),test],axis=1).drop(\"geology_id\",axis=1)\n    \n    train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:52:17.593552Z","iopub.execute_input":"2025-04-24T04:52:17.593835Z","iopub.status.idle":"2025-04-24T04:52:19.141164Z","shell.execute_reply.started":"2025-04-24T04:52:17.593813Z","shell.execute_reply":"2025-04-24T04:52:19.140414Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Add new feature","metadata":{}},{"cell_type":"code","source":"NEW_FEATURE=0#we don't add new features\ndef add_diff_feature(df):\n    cols = [col for col in df.columns if col.lstrip('-').isdigit()]\n    cols = [col for col in cols if -299 <= int(col) <= 0]\n    cols = sorted(cols, key=lambda x: int(x))\n    diff_dict = {}\n    for i in range(0, len(cols)-1,20):\n        col1 = cols[i]\n        col2 = cols[i+1]\n        new_col_name = f\"diff_{col1}_{col2}\"\n        diff_dict[new_col_name] = df[col1].astype(float) - df[col2].astype(float)\n    df_diff = pd.DataFrame(diff_dict)\n    df = pd.concat([df_diff,df], axis=1)\n    print(df_diff.shape)\n    return df\n    \n# train = add_diff_feature(train)\n# test = add_diff_feature(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:52:19.142158Z","iopub.execute_input":"2025-04-24T04:52:19.142370Z","iopub.status.idle":"2025-04-24T04:52:19.148222Z","shell.execute_reply.started":"2025-04-24T04:52:19.142352Z","shell.execute_reply":"2025-04-24T04:52:19.147322Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Score function","metadata":{}},{"cell_type":"code","source":"NEGATIVE_PART = -299\nLARGEST_CHUNK = 600\nTOTAL_REALIZATIONS = 10\nINFLATION_SIGMA = 600\nnum_columns = LARGEST_CHUNK + NEGATIVE_PART - 1 \n\ndef compute_sigma(num: int) -> np.ndarray:\n    sigma = np.ones(num)\n    segments = [\n        (1, 61, 1.0406028049510443, -6.430669850650689),\n        (61, 245, 0.0, -2.1617411566043896),\n        (245, 301, 7.835345062351012, -45.24876794412965)\n    ]\n    for start, end, slope, offset in segments:\n        indices = np.arange(start, end)  \n        sigma[indices - 1] = np.exp(np.log(indices) * slope + offset)\n    return sigma * INFLATION_SIGMA\n\ncov_inv_diag = 1. / compute_sigma(num_columns)\n# ---------------- Simplified code ----------------\ndef score_simplified(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    solution = solution.drop(columns=[row_id_column_name])\n    submission = submission.drop(columns=[row_id_column_name])\n    num_rows = solution.shape[0]\n    inner_product_matrix = np.empty((num_rows, TOTAL_REALIZATIONS))\n    \n    cols_list = [\n        [str(i+1) if k == 0 else f\"r_{k}_pos_{i+1}\" for i in range(num_columns)]\n        for k in range(TOTAL_REALIZATIONS)\n    ]\n    solution_arr = np.stack([solution[cols].values for cols in cols_list], axis=1)\n    submission_arr = np.stack([submission[cols].values for cols in cols_list], axis=1)\n\n    ##core metric\n    # shape=(num_rows, TOTAL_REALIZATIONS, num_columns)\n    misfit = solution_arr - submission_arr\n    inner_product_matrix = np.sum(cov_inv_diag * misfit  * misfit, axis=2)\n    print(logsumexp(inner_product_matrix -np.log(TOTAL_REALIZATIONS), axis=1).shape)\n    score_val = -logsumexp(inner_product_matrix, axis=1).mean()+np.log(TOTAL_REALIZATIONS)\n    ##core metric\n    return score_val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:52:19.149122Z","iopub.execute_input":"2025-04-24T04:52:19.149395Z","iopub.status.idle":"2025-04-24T04:52:19.167749Z","shell.execute_reply.started":"2025-04-24T04:52:19.149360Z","shell.execute_reply":"2025-04-24T04:52:19.166929Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training and model","metadata":{}},{"cell_type":"code","source":"X = np.array(train.iloc[:,0:300+VECTOR_SIZE+NEW_FEATURE]).astype('float32')\ny = np.array(train.iloc[:,300+VECTOR_SIZE+NEW_FEATURE:600+VECTOR_SIZE+NEW_FEATURE]).astype('float32')\nX_test = np.array(test.iloc[:,0:300+VECTOR_SIZE+NEW_FEATURE]).astype('float32')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:52:19.168617Z","iopub.execute_input":"2025-04-24T04:52:19.168862Z","iopub.status.idle":"2025-04-24T04:52:20.079334Z","shell.execute_reply.started":"2025-04-24T04:52:19.168843Z","shell.execute_reply":"2025-04-24T04:52:20.078502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import Loss\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Add,Permute,GlobalAveragePooling1D,Input,GlobalMaxPooling1D,SeparableConv1D, Lambda, Dense, Conv1D, MultiHeadAttention, LayerNormalization, Dropout\nfrom tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\nfrom tensorflow.keras.layers import Input, Dense, Lambda, LayerNormalization, MultiHeadAttention, Conv1D, BatchNormalization, GlobalAveragePooling1D\n\n## custom loss for metric\nclass CustomLoss(Loss):\n    def __init__(self, k_matrix):\n        super().__init__()\n        self.k_matrix = k_matrix\n\n    def call(self, y_true, y_pred):\n        diff = y_true - y_pred\n        loss = diff * tf.convert_to_tensor(self.k_matrix,dtype=tf.float32) * diff\n        return keras.backend.mean(loss)\n        \n## Model Architecture\ndef build_model(input_dim, output_dim):\n    inputs = Input(shape=(input_dim,))\n    col_indices = tf.range(1, 301, dtype=tf.float32)  \n    inputs =  col_indices + inputs  \n    x = Lambda(lambda t: tf.expand_dims(t, axis=1))(inputs) #For Conv1D\n    x = Conv1D(filters=64, kernel_size=7, padding='same', activation='relu')(x) #Need 3d input\n    mha = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n    mha = LayerNormalization()(mha + x)  \n    mha = GlobalAveragePooling1D()(mha)\n    ffn = Dense(512, activation=\"relu\")(mha)\n    ffn = Dense(64, activation=\"relu\")(ffn)\n    ffn = Dense(output_dim, activation=\"linear\")(ffn)\n    model = keras.Model(inputs, ffn)  \n    #print(model.summary())\n    return model\n\ndef cross_validate(X, y, X_test, k_matrix, n_folds=5):\n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    val_losses = []\n    y_oof = np.zeros_like(y, dtype=np.float32)\n    y_test=[]\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n        print(f\"Training on fold {fold + 1}/{n_folds}\")\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n\n        #scale to x\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_val = scaler.transform(X_val)\n        X_test1 = scaler.transform(X_test)\n\n        #scale to y\n        scaler_y = StandardScaler()\n        y_train = scaler_y.fit_transform(y_train)\n        y_val = scaler_y.transform(y_val)\n\n        model = build_model(input_dim=X.shape[1], output_dim=y.shape[1])\n        model.compile(optimizer=Adam(learning_rate=0.0001),\n                      loss=CustomLoss(k_matrix))\n        history = model.fit(X_train, \n                            y_train,\n                            validation_data=(X_val, y_val),\n                            epochs=20,\n                            batch_size=128,\n                            verbose=2)\n\n        val_loss = history.history['val_loss'][-1]\n        val_losses.append(val_loss)\n        print(f\"Fold {fold + 1} validation loss: {val_loss}\")\n        y_oof[val_idx] = np.squeeze(scaler_y.inverse_transform(model.predict(X_val,verbose=0)))\n        y_test.append(np.squeeze(scaler_y.inverse_transform(model.predict(X_test1,verbose=0))))\n    avg_val_loss = np.mean(val_losses)\n    print(f\"Average validation loss across {n_folds} folds: {avg_val_loss}\")\n    return avg_val_loss,y_oof,y_test\n\ninput_dim = 300+VECTOR_SIZE\noutput_dim = 300\n\n_,y_oof,y_test = cross_validate(X, y, X_test, cov_inv_diag)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-04-24T04:52:20.080266Z","iopub.execute_input":"2025-04-24T04:52:20.080656Z","iopub.status.idle":"2025-04-24T04:57:30.739761Z","shell.execute_reply.started":"2025-04-24T04:52:20.080620Z","shell.execute_reply":"2025-04-24T04:57:30.738631Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Repeat\n\nAs mentioned [here](https://www.kaggle.com/competitions/geology-forecast-challenge-open/discussion/569884#3159115), the best strategy is to copy 300 rows ten times.","metadata":{}},{"cell_type":"code","source":"group_size = 300\n\nnum_groups = (train_sub.shape[1] + group_size - 1) // group_size-1\n\nfor i in range(num_groups):\n    start_col = i * group_size+1\n    end_col = (i + 1) * group_size+1\n    train_sub.iloc[:, start_col:end_col] = y_oof","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:57:30.740890Z","iopub.execute_input":"2025-04-24T04:57:30.741734Z","iopub.status.idle":"2025-04-24T04:57:31.245018Z","shell.execute_reply.started":"2025-04-24T04:57:30.741706Z","shell.execute_reply":"2025-04-24T04:57:31.244255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"final CV:{score_simplified(solution,train_sub,'geology_id')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:57:31.245897Z","iopub.execute_input":"2025-04-24T04:57:31.246128Z","iopub.status.idle":"2025-04-24T04:57:47.540278Z","shell.execute_reply.started":"2025-04-24T04:57:31.246108Z","shell.execute_reply":"2025-04-24T04:57:47.539409Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# submission","metadata":{}},{"cell_type":"code","source":"group_size = 300\n\nnum_groups = (sub.shape[1] + group_size - 1) // group_size-1\n\nfor i in range(num_groups):\n    start_col = i * group_size+1\n    end_col = (i + 1) * group_size+1\n    sub.iloc[:, start_col:end_col] = np.mean(y_test,axis=0)\nsub.to_csv(\"submission.csv\",index=None)\nsub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:57:47.541237Z","iopub.execute_input":"2025-04-24T04:57:47.541647Z","iopub.status.idle":"2025-04-24T04:57:49.830793Z","shell.execute_reply.started":"2025-04-24T04:57:47.541606Z","shell.execute_reply":"2025-04-24T04:57:49.829851Z"}},"outputs":[],"execution_count":null}]}